# 会話ログ - ユーザーとAIアーキテクトの会話

**開催日時**: 2025年08月14日 10:38:35
**参加者**: User, AIアーキテクト

---

## 📋 記録

### 1. User

"AIAutoProgramming.pyというプログラムを設計して"

---

### 2. AIアーキテクト

目的
- AIAutoProgramming.py は、要件入力から設計・実装・テスト・レビュー・PR作成までを自動化するマルチエージェント型のAIプログラミングオーケストレータです。CLIとAPIの両形態で提供し、既存リポジトリに非侵襲に統合可能にします。

主なユースケース
- Issue/要件からのコード生成と差分作成
- 既存コードベースのリファクタ・型導入・ライブラリアップグレード
- バグ再現・テスト追加・修正・PR化
- セキュリティスキャンと修正提案の自動化

アーキテクチャ概要（モジュラー・拡張性重視）
- Orchestrator: タスク状態管理・イベント駆動・各エージェント協調
- Agents（プラガブル）:
  - PlannerAgent: 要件解析・作業分解・優先度付け
  - RepoAgent: リポスキャン・依存関係解析・影響範囲特定
  - CodingAgent: コード生成・差分適用・リファクタ
  - TestAgent: 単体/統合テスト生成・実行・修復ループ
  - ReviewAgent: 静的解析・スタイル・セキュリティレビュー
  - CICDAgent: PR作成、CIステータス監視、コメント反映
- Tooling/Adapters:
  - VCS: GitPython、PyGithub/GitLab API
  - Runtime: Docker SDK（安全な実行サンドボックス）
  - LLM Provider: 抽象化（LiteLLM/自作アダプタ）でベンダーロック回避
  - Analyzer: tree-sitter/ast、ruff、black、mypy、pytest、coverage、bandit、semgrep
  - VectorStore/Memo: SQLite/Chromaでリポ要約・RAG
- API/CLI:
  - CLI(Typer): init/plan/implement/test/pr/run-all
  - API(FastAPI, 任意): 外部サービス連携用
- Observability: 構造化ログ(loguru)、OpenTelemetry tracing、Prometheusメトリクス

推奨技術スタック
- Python 3.11+
- 依存: typer, pydantic, httpx, tenacity, asyncio, uvloop, GitPython, docker, python-dotenv, loguru
- LLM: LiteLLM経由で OpenAI(4.1/4o)、Anthropic(Claude 3.5)、Google(Gemini 1.5)、Meta Llama 3.1 等を切替
- RAG: chromadb or sqlite + sentence-transformers（小規模ならsqliteで十分）
- エージェントフレームワーク（選択肢）
  - LangGraph: 明示的な状態遷移で可視化容易（推奨）
  - AutoGen: 迅速だがブラックボックス化しやすい
  - CrewAI: 役割分担が簡潔、シンプル構成向け
  - カスタム（本提案は軽量カスタム + 将来LangGraph移行）

データモデル（pydantic）
- TaskSpec: ユーザー要件、制約、受入基準、ターゲットパス
- Plan: ステップ一覧（id, dependsOn, doneCriteria）
- CodePatch: ファイル差分、ブランチ、コミットメッセージ
- TestResult: 成功/失敗、ログ、カバレッジ、フレーク検知
- ReviewReport: lint/型/セキュリティ/パフォーマンスの指摘
- RunState: 現在のステップ・生成物・再試行回数

実行フロー（自動化優先・品質担保）
1) init: リポクローン/チェック、依存インベントリ、RAGインデックス構築
2) plan: 要件→Plan生成（受入基準をテストに落とす）
3) implement: RAG + リポ分析から最小差分を生成、branch作成・パッチ適用
4) test: Dockerサンドボックスでpytest、失敗時は自己修復ループ（最大N回）
5) review: ruff/black/mypy/semgrep/bandit、修正可能な範囲は自動修正
6) pr: PR作成、説明、チェックリスト、リスク、ロールバック案、CIトリガ
7) monitor: CI失敗を検知して自動追従コミット（オプション）

セキュリティ/プライバシー
- 生成コードの実行はDockerで制限（メモリ/CPU/ネットワーク隔離）
- Secretsは.env or Vault、LLMリクエストに機密投入を禁止（スニペットマスク）
- 依存アップデートはCVE監視（pip-audit、osv-scanner）
- 監査ログの永続化（誰が何を自動生成・適用したか）

CI/CD組込み
- GitHub Actionsテンプレート: lint→type-check→unit-test→sast→coverage→artifact
- PRラベルでの挙動切替（auto-fix, security-only など）
- pre-commit: black, isort, ruff, end-of-file-fixer
- Release: SemVer、GitHub Releases自動ノート生成

パフォーマンス最適化
- RAGによる対象ファイル選別（トークン節約）
- 依存グラフで影響範囲限定（pytest -k/--maxfail）
- httpx + asyncio + uvloopで並列I/O
- LLMレスポンスキャッシュ（diskcache/redis）
- ログ・テレメトリのサンプリング

AIAutoProgramming.py（最小実装スケルトン）
- 単一ファイルでのPoC開始。後続でパッケージ分割を前提。

```python
# AIAutoProgramming.py
import asyncio, os, json, subprocess, tempfile, sys
from typing import List, Optional, Dict, Any
import typer, httpx
from pydantic import BaseModel, Field
from loguru import logger

app = typer.Typer(add_completion=False)

# -------- Config --------
class LLMConfig(BaseModel):
    provider: str = "openai"
    model: str = "gpt-4.1-mini"
    api_key_env: str = "OPENAI_API_KEY"
    max_tokens: int = 4096
    temperature: float = 0.2

class AppConfig(BaseModel):
    repo_dir: str = "."
    llm: LLMConfig = LLMConfig()
    sandbox_image: str = "python:3.11-slim"
    max_retries: int = 3

# -------- Data Models --------
class TaskSpec(BaseModel):
    title: str
    description: str
    acceptance: List[str] = Field(default_factory=list)
    target_paths: List[str] = Field(default_factory=list)

class PlanStep(BaseModel):
    id: str
    desc: str
    depends_on: List[str] = Field(default_factory=list)

class Plan(BaseModel):
    steps: List[PlanStep]

class CodePatch(BaseModel):
    branch: str
    commits: List[Dict[str, Any]]

class TestResult(BaseModel):
    success: bool
    stdout: str
    stderr: str

# -------- LLM Adapter (simplified) --------
async def llm_complete(cfg: LLMConfig, messages: List[Dict[str, str]]) -> str:
    api_key = os.getenv(cfg.api_key_env)
    if not api_key:
        raise RuntimeError(f"Missing API key in {cfg.api_key_env}")
    # Example: LiteLLM style endpoint could be used; here a generic placeholder
    url = os.getenv("LLM_PROXY_URL", "http://localhost:4000/v1/chat/completions")
    payload = {"model": cfg.model, "messages": messages, "temperature": cfg.temperature}
    async with httpx.AsyncClient(timeout=60) as client:
        r = await client.post(url, headers={"Authorization": f"Bearer {api_key}"}, json=payload)
        r.raise_for_status()
        data = r.json()
        return data["choices"][0]["message"]["content"]

# -------- Tools --------
def run_cmd(cmd: List[str], cwd: Optional[str] = None) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, cwd=cwd, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

def docker_pytest(repo_dir: str, image: str) -> TestResult:
    cmd = [
        "docker","run","--rm",
        "-v", f"{os.path.abspath(repo_dir)}:/work",
        "-w","/work", image,"bash","-lc",
        "pip install -U pip && pip install -r requirements.txt || true && pytest -q"
    ]
    p = run_cmd(cmd)
    return TestResult(success=(p.returncode==0), stdout=p.stdout, stderr=p.stderr)

# -------- Agents (simplified) --------
class PlannerAgent:
    async def plan(self, cfg: AppConfig, spec: TaskSpec, repo_summary: str) -> Plan:
        prompt = f"""You are a senior planner. Repo summary: {repo_summary}
Task: {spec.title}\nDesc: {spec.description}\nAcceptance: {spec.acceptance}
Produce 3-6 concrete steps with dependencies. Return JSON with steps."""
        out = await llm_complete(cfg.llm, [{"role":"user","content":prompt}])
        try:
            data = json.loads(out)
            steps = [PlanStep(**s) for s in data["steps"]]
        except Exception:
            # fallback minimal plan
            steps = [PlanStep(id="impl", desc="Implement changes", depends_on=[]),
                     PlanStep(id="test", desc="Add and run tests", depends_on=["impl"])]
        return Plan(steps=steps)

class CodingAgent:
    async def implement(self, cfg: AppConfig, spec: TaskSpec, plan: Plan) -> CodePatch:
        # Very simplified: request unified diff
        prompt = f"""Generate a minimal git unified diff for repo to satisfy:
{spec.description}
Only output a valid unified diff."""
        diff = await llm_complete(cfg.llm, [{"role":"user","content":prompt}])
        branch = f"ai/{spec.title.replace(' ','-')}"
        run_cmd(["git","checkout","-b",branch], cwd=cfg.repo_dir)
        with tempfile.NamedTemporaryFile("w", delete=False) as f:
            f.write(diff)
            path = f.name
        apply = run_cmd(["git","apply","--whitespace=fix", path], cwd=cfg.repo_dir)
        if apply.returncode != 0:
            logger.error(apply.stderr)
        run_cmd(["git","add","-A"], cwd=cfg.repo_dir)
        run_cmd(["git","commit","-m", f"feat: {spec.title}\n\n[auto]"], cwd=cfg.repo_dir)
        return CodePatch(branch=branch, commits=[{"message": spec.title}])

class TestAgent:
    def run(self, cfg: AppConfig) -> TestResult:
        return docker_pytest(cfg.repo_dir, cfg.sandbox_image)

# -------- Orchestrator --------
async def run_all(cfg: AppConfig, spec: TaskSpec):
    # 1) repo summary (very rough for PoC)
    files = run_cmd(["git","ls-files"], cwd=cfg.repo_dir).stdout.splitlines()[:200]
    repo_summary = f"Files: {files[:20]}"
    # 2) plan
    plan = await PlannerAgent().plan(cfg, spec, repo_summary)
    logger.info(f"Plan: {[s.id for s in plan.steps]}")
    # 3) implement
    patch = await CodingAgent().implement(cfg, spec, plan)
    logger.info(f"Branch: {patch.branch}")
    # 4) test
    result = TestAgent().run(cfg)
    logger.info(f"Tests success={result.success}")
    if not result.success:
        logger.error(result.stderr[:2000])
    return result.success

# -------- CLI --------
@app.command()
def init(repo: str = typer.Option(".", help="Repo directory")):
    cfg = AppConfig(repo_dir=repo)
    typer.echo(f"Initialized for {os.path.abspath(repo)}")

@app.command()
def plan(title: str, desc: str):
    cfg = AppConfig()
    spec = TaskSpec(title=title, description=desc)
    asyncio.run(PlannerAgent().plan(cfg, spec, ""))
    typer.echo("Plan created (see logs).")

@app.command("run-all")
def run_all_cmd(title: str, desc: str):
    cfg = AppConfig()
    spec = TaskSpec(title=title, description=desc)
    ok = asyncio.run(run_all(cfg, spec))
    sys.exit(0 if ok else 1)

if __name__ == "__main__":
    app()
```

ディレクトリ構成（将来の分割）
- /AIAutoProgramming.py（エントリ）
- /agents/{planner.py,coding.py,test.py,review.py}
- /tools/{llm.py,git.py,docker.py,analyzer.py}
- /adapters/{github.py,gitlab.py}
- /prompts/{planner.md,coder.md,review.md}
- /configs/config.yaml
- /tests/...

GitHub連携（概要）
- PyGithubでPR作成、ステータス取得、コメント投稿
- チェックリスト付きPR本文生成（要件・影響範囲・テスト結果・ロールバック手順）
- ラベル: ai-generated, needs-review, security

品質管理
- 失敗時の自動修復ループ: 上限回数、エラー要約、差分縮小
- フレークテスト検出: 再試行とpytest -x/--maxfail で加速
- 生成差分のサイズ/リスクガードレール（大規模変更はhuman-in-the-loop）

選択肢と比較
- LangGraph採用: 明確な状態遷移・再現性◎、学習コスト中
- AutoGen採用: 実装速度◎、デバッグ難易度△
- 純Pythonカスタム: 透過性◎、機能実装の手間△
- LLMプロバイダ: 商用(品質◎コスト△) vs OSS(コスト◎品質△)。LiteLLMで抽象化推奨

プロトタイプ→本格実装ロードマップ
- Week 1: PoC（CLI run-all、OpenAI/Claude切替、Dockerでpytest、PR作成）
- Week 2-3: RAG導入、LangGraph化、自己修復ループ強化、静的解析統合、観測基盤
- Week 4: セキュリティ強化（sandbox/secret masking）、組織CIテンプレ配布、導入ガイド
- 継続: メトリクス（成功率/試行回数/修正規模）収集→プロンプト/ポリシー改善

セキュリティ/コンプライアンスの注意
- 社外LLM利用時はソースコードの取り扱い方針を設定（PII/Secretsマスク）
- 監査可能なログと変更トレーサビリティの保持
- ライセンス遵守（SPDX、依存監査）

この設計をベースに、まずは提供のスケルトンでPoCを動かし、段階的にエージェント分割・LangGraph導入・RAG強化・CI統合を行うと、安全かつ拡張性高く展開できます。必要であれば、特定のリポ/CI環境（GitHub/GitLab/Bitbucket）向けの具体的なアダプタ実装も提示します。

---

