#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Assistants Configuration Updater
å„AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ã—ã¦ã€ai_assistants_config.csvã‚’æ›´æ–°ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ©Ÿèƒ½:
- å„AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®APIã‹ã‚‰æœ€æ–°ã®åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è‡ªå‹•å–å¾—
-             print(f"                # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨                print(f"    âœ“ ä¸»è¦ãƒ¢ãƒ‡ãƒ«: {sorted(important_models)}")
                if other_count > 0:
                    print(f"    âœ“ ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«: {other_count}å€‹ (VERBOSE_MODE=true ã§å…¨è¡¨ç¤º)")             if other_count > 0:
                    print(f"    âœ“ ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«: {other_count}å€‹ (VERBOSE_MODE=true ã§å…¨è¡¨ç¤º)")å¤šã™ãã‚‹å ´åˆã¯è¦ç´„ï¼‰
            if verbose_mode:
                print(f"    âœ“ å…¨GPTãƒ¢ãƒ‡ãƒ«ä¸€è¦§: {sorted(gpt_models)}")
            elif len(gpt_models) <= 10:
                print(f"    âœ“ ç™ºè¦‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {gpt_models}")
            else:
                # ä¸»è¦ãƒ¢ãƒ‡ãƒ«ã®ã¿è¡¨ç¤º
                important_models = [m for m in gpt_models if any(keyword in m for keyword in ['gpt-5', 'gpt-4o', 'gpt-4-turbo'])]
                other_count = len(gpt_models) - len(important_models)
                print(f"    âœ“ ä¸»è¦ãƒ¢ãƒ‡ãƒ«: {sorted(important_models)}")
                if other_count > 0:
                    print(f"    âœ“ ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«: {other_count}å€‹ (VERBOSE_MODE=true ã§å…¨è¡¨ç¤º)")                   print(f"    âœ“ ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«: {other_count}å€‹ (VERBOSE_MODE=true ã§å…¨è¡¨ç¤º)")           print(f"    âœ“ ä¸»è¦ãƒ¢ãƒ‡ãƒ«: {sorted(important_models)}")
                if other_count > 0:
                    print(f"    âœ“ ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«: {other_count}å€‹ (VERBOSE_MODE=true ã§å…¨è¡¨ç¤º)")             if other_count > 0:
                    print(f"    âœ“ ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«: {other_count}å€‹ (VERBOSE_MODE=true ã§å…¨è¡¨ç¤º)"){len(gpt_models)} å€‹ã®GPTãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹")
            
            # è©³ç´°è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆç’°å¢ƒå¤‰æ•°VERBOSE_MODEãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
            verbose_mode = os.getenv('VERBOSE_MODE', '').lower() in ['true', '1', 'yes']
            
            # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤ºï¼ˆå¤šã™ãã‚‹å ´åˆã¯è¦ç´„ï¼‰
            if verbose_mode:
                print(f"    âœ“ å…¨GPTãƒ¢ãƒ‡ãƒ«ä¸€è¦§: {sorted(gpt_models)}")
            elif len(gpt_models) <= 10:
                print(f"    âœ“ ç™ºè¦‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {gpt_models}")
            else:
                # ä¸»è¦ãƒ¢ãƒ‡ãƒ«ã®ã¿è¡¨ç¤º
                important_models = [m for m in gpt_models if any(keyword in m for keyword in ['gpt-5', 'gpt-4o', 'gpt-4-turbo'])]
                other_count = len(gpt_models) - len(important_models)
                print(f"    âœ“ ä¸»è¦ãƒ¢ãƒ‡ãƒ«: {sorted(important_models)}")
                if other_count > 0:
                    print(f"    âœ“ ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«: {other_count}å€‹ (VERBOSE_MODE=true ã§å…¨è¡¨ç¤º)")ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ç™ºè¦‹
- é€šå¸¸ãƒ¢ãƒ‡ãƒ«(model)ã¨é«˜é€Ÿãƒ¢ãƒ‡ãƒ«(fast_model)ã®ä¸¡æ–¹ã«å¯¾å¿œ
- APIå–å¾—å¤±æ•—æ™‚ã¯äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨å®‰å…¨ãªæ›´æ–°å‡¦ç†
- ãƒ¢ãƒ‡ãƒ«ã‚³ã‚¹ãƒˆæƒ…å ±ã®ç®¡ç†ã¨ã‚¯ãƒ­ãƒ¼ãƒ«æ©Ÿèƒ½ã‚’çµ±åˆ

å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:
- OpenAI (ChatGPT): gpt-5, gpt-4o, gpt-4o-mini ãªã© (API + Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°)
- Google (Gemini): gemini-2.5-pro, gemini-2.5-flash ãªã© (API)
- Groq: llama-3.3-70b-versatile, llama-3.1-8b-instant ãªã© (API)
- Mistral AI: mistral-large-latest, mistral-small-latest ãªã© (API)
- Anthropic (Claude): claude-opus-4, claude-3-5-haiku-latest ãªã© (API + Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°)
- Together AI: meta-llama ãƒ¢ãƒ‡ãƒ«ç¾¤ (Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
- xAI (Grok): grok-3-latest ãªã© (API + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)

ã‚³ãƒãƒ³ãƒ‰:
  python update_ai_config.py [update]                                            # AIè¨­å®šã®æ›´æ–°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
  python update_ai_config.py costs list                                          # ã‚³ã‚¹ãƒˆä¸€è¦§è¡¨ç¤º
  python update_ai_config.py costs verify                                        # AIè¨­å®šã¨ã‚³ã‚¹ãƒˆDBã®æ•´åˆæ€§ç¢ºèª
  python update_ai_config.py costs add <model> <provider> <input> <output>       # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«è¿½åŠ 
  python update_ai_config.py costs update <model> <input> <output>               # ã‚³ã‚¹ãƒˆæ›´æ–°
  python update_ai_config.py costs crawl <source>                                # å¤–éƒ¨APIã‹ã‚‰ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«

Crawl sources: openai, anthropic, google, groq, all
"""

import requests
import csv
import json
import os
from datetime import datetime
import time
import sys
import re
from typing import Dict, List, Optional

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
CONFIG_FILE = "ai_assistants_config.csv"
MODEL_COSTS_FILE = "model_costs.csv"
BACKUP_FILE = f"ai_assistants_config_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

# å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«è¨­å®š
def get_fallback_models():
    """ç¾åœ¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‹•çš„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ç”Ÿæˆ"""
    fallback_models = {}
    
    # ã¾ãšç¾åœ¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    if os.path.exists(CONFIG_FILE):
        try:
            print(f"[INFO] æ—¢å­˜ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {CONFIG_FILE} ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    assistant_name = row['assistant_name']
                    fallback_models[assistant_name] = {
                        'module': row['module'],
                        'class': row['class'],
                        'model': row['model'],
                        'fast_model': row.get('fast_model', ''),
                    }
                    print(f"  âœ“ {assistant_name}: {row['model']} / {row.get('fast_model', 'ãªã—')}")
            
            print(f"[INFO] {len(fallback_models)} å€‹ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"[WARNING] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("[INFO] ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™")
            fallback_models = get_hardcoded_fallback_models()
    else:
        print(f"[INFO] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {CONFIG_FILE} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚åˆæœŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™")
        fallback_models = get_hardcoded_fallback_models()
    
    return fallback_models

def get_hardcoded_fallback_models():
    """ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸåˆæœŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆç”¨ï¼‰"""
    return {
        'ChatGPT': {
            'module': 'langchain_openai',
            'class': 'ChatOpenAI',
            'model': 'gpt-5',  # é«˜æ€§èƒ½åŸºæœ¬ç‰ˆ
            'fast_model': 'gpt-5-chat-latest',  # ãƒãƒ£ãƒƒãƒˆç‰¹åŒ–é«˜é€Ÿç‰ˆ
        },
        'Gemini': {
            'module': 'langchain_google_genai',
            'class': 'ChatGoogleGenerativeAI',
            'model': 'gemini-2.5-pro',  # æœ€æ–°å®‰å®šç‰ˆ
            'fast_model': 'gemini-2.5-flash',  # é«˜é€Ÿç‰ˆ
        },
        'Groq': {
            'module': 'langchain_groq',
            'class': 'ChatGroq',
            'model': 'llama-3.3-70b-versatile',  # æœ€æ–°å¤§å‹ãƒ¢ãƒ‡ãƒ«
            'fast_model': 'llama-3.1-8b-instant',  # é«˜é€Ÿç‰ˆ
        },
        'Mistral': {
            'module': 'langchain_mistralai',
            'class': 'ChatMistralAI',
            'model': 'mistral-large-latest',  # æœ€æ–°å¤§å‹ãƒ¢ãƒ‡ãƒ«
            'fast_model': 'mistral-small-latest',  # é«˜é€Ÿç‰ˆ
        },
        'Together': {
            'module': 'langchain_together',
            'class': 'ChatTogether',
            'model': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8',
            'fast_model': '',  # é«˜é€Ÿç‰ˆï¼ˆç©ºç™½ï¼‰
        },
        'Anthropic': {
            'module': 'langchain_anthropic',
            'class': 'ChatAnthropic',
            'model': 'claude-opus-4-20250514',  # æœ€æ–°Claude 4
            'fast_model': 'claude-3-5-haiku-latest',  # é«˜é€Ÿç‰ˆ
        },
        'Grok': {
            'module': 'langchain_xai',
            'class': 'ChatXAI',
            'model': 'grok-4-0709',  # æœ€æ–°Grok 4
            'fast_model': 'grok-3-mini-fast',  # é«˜é€Ÿç‰ˆ
        }
    }

# å‹•çš„ã«è¨­å®šã‚’å–å¾—
FALLBACK_MODELS = get_fallback_models()

def backup_config_file():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã€åŒæ™‚ã«ç¾åœ¨ã®è¨­å®šã‚’FALLBACK_MODELSã«åæ˜ ã™ã‚‹"""
    global FALLBACK_MODELS
    
    if os.path.exists(CONFIG_FILE):
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰ã«ç¾åœ¨ã®è¨­å®šã‚’èª­ã¿è¾¼ã‚“ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨
            print(f"[INFO] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰ã«ç¾åœ¨ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿ä¸­...")
            FALLBACK_MODELS = get_fallback_models()
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
            with open(CONFIG_FILE, 'r', encoding='utf-8') as src:
                with open(BACKUP_FILE, 'w', encoding='utf-8') as dst:
                    dst.write(src.read())
            print(f"[INFO] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ: {BACKUP_FILE}")
            
            # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆæœ€æ–°3ã¤ã‚’ä¿æŒï¼‰
            cleanup_old_backups()
            
            return True
        except Exception as e:
            print(f"[ERROR] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šã¯åˆæœŸåŒ–
            FALLBACK_MODELS = get_fallback_models()
            return False
    else:
        print(f"[WARNING] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {CONFIG_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯åˆæœŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        FALLBACK_MODELS = get_fallback_models()
        return False

def cleanup_old_backups(keep_count=3):
    """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹ï¼ˆæœ€æ–°ã®Nå€‹ã‚’ä¿æŒï¼‰"""
    try:
        import glob
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        pattern = "ai_assistants_config_backup_*.csv"
        backup_files = glob.glob(pattern)
        
        if len(backup_files) <= keep_count:
            return  # ä¿æŒæ•°ä»¥ä¸‹ãªã‚‰å‰Šé™¤ã—ãªã„
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆæ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        backup_files.sort(key=os.path.getctime, reverse=True)
        
        # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        files_to_delete = backup_files[keep_count:]
        for file_path in files_to_delete:
            try:
                os.remove(file_path)
                print(f"[INFO] å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤: {file_path}")
            except Exception as e:
                print(f"[WARNING] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—: {file_path} - {e}")
                
    except Exception as e:
        print(f"[WARNING] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

# ================== ã‚³ã‚¹ãƒˆç®¡ç†æ©Ÿèƒ½ ==================

def load_ai_assistants_config():
    """AI ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
    config = {}
    if not os.path.exists(CONFIG_FILE):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ« {CONFIG_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return {}
    
    try:
        with open(CONFIG_FILE, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                config[row['assistant_name']] = {
                    'module': row['module'],
                    'class': row['class'],
                    'model': row['model'],
                    'fast_model': row['fast_model']
                }
        return config
    except Exception as e:
        print(f"âŒ AIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def get_provider_from_module(module_name):
    """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‹ã‚‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã‚’æ¨å®š"""
    module_to_provider = {
        'langchain_openai': 'OpenAI',
        'langchain_google_genai': 'Google',
        'langchain_groq': 'Groq',
        'langchain_anthropic': 'Anthropic',
        'langchain_mistralai': 'Mistral',
        'langchain_together': 'Together',
        'langchain_xai': 'XAI'
    }
    return module_to_provider.get(module_name, 'Unknown')

def load_model_costs():
    """ãƒ¢ãƒ‡ãƒ«ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’èª­ã¿è¾¼ã‚€"""
    costs = []
    if not os.path.exists(MODEL_COSTS_FILE):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ« {MODEL_COSTS_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []
    
    try:
        with open(MODEL_COSTS_FILE, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            costs = list(reader)
        return costs
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def save_model_costs(costs):
    """ãƒ¢ãƒ‡ãƒ«ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’ä¿å­˜ã™ã‚‹"""
    try:
        with open(MODEL_COSTS_FILE, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['date_updated', 'provider', 'model', 'input_cost_per_1k_tokens', 'output_cost_per_1k_tokens', 'currency', 'notes']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(costs)
        print(f"âœ… {MODEL_COSTS_FILE} ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def list_costs():
    """ã‚³ã‚¹ãƒˆä¸€è¦§ã‚’è¡¨ç¤º"""
    costs = load_model_costs()
    if not costs:
        return
    
    print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚³ã‚¹ãƒˆä¸€è¦§:")
    print("=" * 100)
    print(f"{'Provider':<15} {'Model':<25} {'Input/1K':<10} {'Output/1K':<10} {'Updated':<12} {'Notes'}")
    print("-" * 100)
    
    for cost in costs:
        provider = cost['provider'][:14]
        model = cost['model'][:24]
        input_cost = f"${cost['input_cost_per_1k_tokens']}"
        output_cost = f"${cost['output_cost_per_1k_tokens']}"
        updated = cost['date_updated']
        notes = cost['notes'][:30]
        
        print(f"{provider:<15} {model:<25} {input_cost:<10} {output_cost:<10} {updated:<12} {notes}")

def verify_provider_mapping():
    """AIè¨­å®šã¨ã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã®æ•´åˆæ€§ã‚’ç¢ºèª"""
    print("\nğŸ” ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ç¢ºèª:")
    print("=" * 80)
    
    ai_config = load_ai_assistants_config()
    costs = load_model_costs()
    
    if not ai_config:
        print("âŒ AIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    print(f"{'Assistant':<12} {'Module':<25} {'Inferred Provider':<15} {'Models in Cost DB'}")
    print("-" * 80)
    
    for assistant_name, config in ai_config.items():
        module = config['module']
        inferred_provider = get_provider_from_module(module)
        
        # ã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§è©²å½“ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
        matching_models = [c['model'] for c in costs if c['provider'] == inferred_provider]
        models_count = len(matching_models)
        
        print(f"{assistant_name:<12} {module:<25} {inferred_provider:<15} {models_count} models")
        
        # ä¸æ•´åˆã‚’ãƒã‚§ãƒƒã‚¯
        assistant_models = [config['model'], config['fast_model']]
        for model in assistant_models:
            if model and not any(c['model'] == model and c['provider'] == inferred_provider for c in costs):
                print(f"  âš ï¸  Missing: {model} for {inferred_provider}")
    
    print()
    return True

def add_model_cost(model, provider, input_cost, output_cost, notes=""):
    """æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ """
    costs = load_model_costs()
    
    # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯
    for cost in costs:
        if cost['model'] == model:
            print(f"âš ï¸  ãƒ¢ãƒ‡ãƒ« '{model}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚updateã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
            return
    
    new_cost = {
        'date_updated': datetime.now().strftime("%Y-%m-%d"),
        'provider': provider,
        'model': model,
        'input_cost_per_1k_tokens': str(input_cost),
        'output_cost_per_1k_tokens': str(output_cost),
        'currency': 'USD',
        'notes': notes
    }
    
    costs.append(new_cost)
    save_model_costs(costs)
    print(f"âœ… ãƒ¢ãƒ‡ãƒ« '{model}' ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")

def update_model_cost(model, input_cost, output_cost):
    """æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¹ãƒˆã‚’æ›´æ–°"""
    costs = load_model_costs()
    
    updated = False
    for cost in costs:
        if cost['model'] == model:
            cost['date_updated'] = datetime.now().strftime("%Y-%m-%d")
            cost['input_cost_per_1k_tokens'] = str(input_cost)
            cost['output_cost_per_1k_tokens'] = str(output_cost)
            updated = True
            break
    
    if updated:
        save_model_costs(costs)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ« '{model}' ã®ã‚³ã‚¹ãƒˆã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
    else:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

def update_or_add_model_cost(model, provider, input_cost, output_cost, notes=""):
    """ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯æ›´æ–°ã€å­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ """
    costs = load_model_costs()
    
    # æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
    found = False
    for cost in costs:
        if cost["model"] == model and cost["provider"] == provider:
            cost["input_cost_per_1k_tokens"] = input_cost
            cost["output_cost_per_1k_tokens"] = output_cost
            cost["date_updated"] = datetime.now().strftime("%Y-%m-%d")
            cost["notes"] = notes
            found = True
            break
    
    if not found:
        # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
        costs.append({
            "date_updated": datetime.now().strftime("%Y-%m-%d"),
            "provider": provider,
            "model": model,
            "input_cost_per_1k_tokens": input_cost,
            "output_cost_per_1k_tokens": output_cost,
            "currency": "USD",
            "notes": notes
        })
    
    save_model_costs(costs)

def crawl_openai_costs():
    """OpenAI APIã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—"""
    print("ğŸ“¡ OpenAI APIæƒ…å ±ã‚’å–å¾—ä¸­...")
    
    # OpenAIå…¬å¼ä¾¡æ ¼æƒ…å ±ï¼ˆæ‰‹å‹•æ›´æ–°ãŒå¿…è¦ï¼‰
    openai_models = {
        "gpt-4o": {"input": 2.50, "output": 10.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-4-turbo": {"input": 10.00, "output": 30.00},
        "gpt-4": {"input": 30.00, "output": 60.00},
        "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
        "text-embedding-3-large": {"input": 0.13, "output": 0.13},
        "text-embedding-3-small": {"input": 0.02, "output": 0.02},
        "text-embedding-ada-002": {"input": 0.10, "output": 0.10}
    }
    
    for model, costs in openai_models.items():
        update_or_add_model_cost(model, "OpenAI", costs["input"], costs["output"], 
                           f"Updated via API crawl on {datetime.now().strftime('%Y-%m-%d')}")
    
    print(f"âœ… OpenAI {len(openai_models)} ãƒ¢ãƒ‡ãƒ«ã®ä¾¡æ ¼ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

def crawl_anthropic_costs():
    """Anthropic APIã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—"""
    print("ğŸ“¡ Anthropic APIæƒ…å ±ã‚’å–å¾—ä¸­...")
    
    # Anthropicå…¬å¼ä¾¡æ ¼æƒ…å ±ï¼ˆæ‰‹å‹•æ›´æ–°ãŒå¿…è¦ï¼‰
    anthropic_models = {
        "claude-3-5-sonnet-20241022": {"input": 3.00, "output": 15.00},
        "claude-3-5-haiku-20241022": {"input": 0.25, "output": 1.25},
        "claude-3-opus-20240229": {"input": 15.00, "output": 75.00},
        "claude-3-sonnet-20240229": {"input": 3.00, "output": 15.00},
        "claude-3-haiku-20240307": {"input": 0.25, "output": 1.25}
    }
    
    for model, costs in anthropic_models.items():
        update_or_add_model_cost(model, "Anthropic", costs["input"], costs["output"], 
                           f"Updated via API crawl on {datetime.now().strftime('%Y-%m-%d')}")
    
    print(f"âœ… Anthropic {len(anthropic_models)} ãƒ¢ãƒ‡ãƒ«ã®ä¾¡æ ¼ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

def crawl_google_costs():
    """Google AI APIã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—"""
    print("ğŸ“¡ Google AI APIæƒ…å ±ã‚’å–å¾—ä¸­...")
    
    # Google AIå…¬å¼ä¾¡æ ¼æƒ…å ±ï¼ˆæ‰‹å‹•æ›´æ–°ãŒå¿…è¦ï¼‰
    google_models = {
        "gemini-2.0-flash-exp": {"input": 0.075, "output": 0.30},
        "gemini-1.5-pro": {"input": 1.25, "output": 5.00},
        "gemini-1.5-flash": {"input": 0.075, "output": 0.30},
        "gemini-1.0-pro": {"input": 0.50, "output": 1.50}
    }
    
    for model, costs in google_models.items():
        update_or_add_model_cost(model, "Google", costs["input"], costs["output"], 
                           f"Updated via API crawl on {datetime.now().strftime('%Y-%m-%d')}")
    
    print(f"âœ… Google {len(google_models)} ãƒ¢ãƒ‡ãƒ«ã®ä¾¡æ ¼ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

def crawl_groq_costs():
    """Groq APIã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—"""
    print("ğŸ“¡ Groq APIæƒ…å ±ã‚’å–å¾—ä¸­...")
    
    # Groqå…¬å¼ä¾¡æ ¼æƒ…å ±ï¼ˆæ‰‹å‹•æ›´æ–°ãŒå¿…è¦ï¼‰
    groq_models = {
        "llama-3.3-70b-versatile": {"input": 0.59, "output": 0.79},
        "llama-3.1-70b-versatile": {"input": 0.59, "output": 0.79},
        "llama-3.1-8b-instant": {"input": 0.05, "output": 0.08},
        "mixtral-8x7b-32768": {"input": 0.24, "output": 0.24},
        "gemma-7b-it": {"input": 0.07, "output": 0.07}
    }
    
    for model, costs in groq_models.items():
        update_or_add_model_cost(model, "Groq", costs["input"], costs["output"], 
                           f"Updated via API crawl on {datetime.now().strftime('%Y-%m-%d')}")
    
    print(f"âœ… Groq {len(groq_models)} ãƒ¢ãƒ‡ãƒ«ã®ä¾¡æ ¼ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

def crawl_costs(source):
    """å¤–éƒ¨APIã‹ã‚‰ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦æ›´æ–°ã™ã‚‹"""
    print(f"ğŸ” {source} ã‹ã‚‰ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã„ã¾ã™...")
    
    if source == "openai" or source == "all":
        crawl_openai_costs()
    
    if source == "anthropic" or source == "all":
        crawl_anthropic_costs()
    
    if source == "google" or source == "all":
        crawl_google_costs()
    
    if source == "groq" or source == "all":
        crawl_groq_costs()
    
    print("âœ… ã‚³ã‚¹ãƒˆã‚¯ãƒ­ãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# ================== AIè¨­å®šç®¡ç†æ©Ÿèƒ½ ==================

def get_model_priority_score(model_name: str, provider: str) -> int:
    """ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰å„ªå…ˆé †ä½ã‚¹ã‚³ã‚¢ã‚’å‹•çš„ã«è¨ˆç®—"""
    score = 0
    model_lower = model_name.lower()
    
    if provider == 'openai':
        # GPT-5ç³»ãŒæœ€å„ªå…ˆ
        if 'gpt-5' in model_lower:
            score += 1000
            # åŸºæœ¬ã®gpt-5ã‚’é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æœ€å„ªå…ˆ
            if model_lower == 'gpt-5':
                score += 300  # åŸºæœ¬ç‰ˆã‚’æœ€å„ªå…ˆ
            elif 'chat-latest' in model_lower:
                score += 100  # ãƒãƒ£ãƒƒãƒˆç‰¹åŒ–ã¯é«˜é€Ÿç‰ˆå‘ã‘ï¼ˆã‚¹ã‚³ã‚¢ä½ã‚ï¼‰
            elif 'mini' in model_lower or 'nano' in model_lower:
                score += 50   # è»½é‡ç‰ˆã¯é«˜é€Ÿç‰ˆå‘ã‘ï¼ˆã‚¹ã‚³ã‚¢ä½ã‚ï¼‰
            elif any(date in model_lower for date in ['2025-08', '2025-07', '2025-06']):
                score += 150  # æœ€æ–°æ—¥ä»˜ç‰ˆã¯ä¸­ç¨‹åº¦
        elif 'gpt-4o' in model_lower:
            score += 800
            if 'mini' in model_lower:
                score += 50
        elif 'gpt-4' in model_lower:
            score += 600
            if 'turbo' in model_lower:
                score += 100
        
    elif provider == 'anthropic':
        # Claude 4ç³»ãŒæœ€å„ªå…ˆ
        if 'claude-opus-4' in model_lower or 'claude-4-opus' in model_lower:
            score += 1000
        elif 'claude-sonnet-4' in model_lower or 'claude-4-sonnet' in model_lower:
            score += 950
        elif 'claude-3-7' in model_lower:
            score += 900
        elif 'claude-3-5' in model_lower:
            score += 850
            if 'haiku' in model_lower:
                score += 100  # é«˜é€Ÿç‰ˆãƒœãƒ¼ãƒŠã‚¹
        elif 'claude-3' in model_lower:
            score += 800
        
        # æ—¥ä»˜ã®æ–°ã—ã•ã‚’è€ƒæ…®
        if '2025' in model_lower:
            score += 200
        elif '2024' in model_lower:
            score += 100
            
    elif provider == 'grok':
        # Grok 4ç³»ãŒæœ€å„ªå…ˆ
        if 'grok-4' in model_lower:
            score += 1000
            # åŸºæœ¬ã®grok-4ã‚’é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æœ€å„ªå…ˆ
            if 'grok-4' in model_lower and len(model_lower.replace('grok-4', '').replace('-', '')) <= 10:
                score += 200  # åŸºæœ¬ç‰ˆã¾ãŸã¯ã‚·ãƒ³ãƒ—ãƒ«ãªæ—¥ä»˜ç‰ˆã‚’æœ€å„ªå…ˆ
        elif 'grok-3' in model_lower:
            score += 900
            if 'mini' in model_lower and 'fast' in model_lower:
                score += 50  # é«˜é€Ÿç‰ˆã¯é«˜é€Ÿãƒ¢ãƒ‡ãƒ«å‘ã‘ï¼ˆã‚¹ã‚³ã‚¢ä½ã‚ï¼‰
            elif 'fast' in model_lower:
                score += 30  # é«˜é€Ÿç‰ˆã¯é«˜é€Ÿãƒ¢ãƒ‡ãƒ«å‘ã‘ï¼ˆã‚¹ã‚³ã‚¢ä½ã‚ï¼‰
            elif 'mini' in model_lower:
                score += 20  # è»½é‡ç‰ˆã¯é«˜é€Ÿãƒ¢ãƒ‡ãƒ«å‘ã‘ï¼ˆã‚¹ã‚³ã‚¢ä½ã‚ï¼‰
        elif 'grok-2' in model_lower:
            score += 800
        
        # æ—¥ä»˜ã®æ–°ã—ã•ã‚’è€ƒæ…®
        if '2025' in model_lower or '1212' in model_lower:
            score += 100
            
    elif provider == 'gemini':
        # Gemini 2.5ç³»ãŒæœ€å„ªå…ˆ
        if 'gemini-2.5' in model_lower:
            score += 1000
            if 'pro' in model_lower:
                score += 100  # Proã¯é«˜æ€§èƒ½ç‰ˆ
            elif 'flash' in model_lower:
                score += 50   # Flashã¯é«˜é€Ÿç‰ˆï¼ˆã‚¹ã‚³ã‚¢ä½ã‚ï¼‰
        elif 'gemini-2.0' in model_lower:
            score += 950
        elif 'gemini-1.5' in model_lower:
            score += 900
        elif 'gemini-1.0' in model_lower:
            score += 800
            
    elif provider == 'groq':
        # Llama 3.3ç³»ãŒæœ€å„ªå…ˆ
        if 'llama-3.3' in model_lower:
            score += 1000
        elif 'llama-3.2' in model_lower:
            score += 950
        elif 'llama-3.1' in model_lower:
            score += 900
        elif 'llama-3' in model_lower:
            score += 850
        
        # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º
        if '70b' in model_lower:
            score += 100
        elif '8b' in model_lower:
            score += 50
            if 'instant' in model_lower:
                score += 100  # é«˜é€Ÿç‰ˆãƒœãƒ¼ãƒŠã‚¹
                
    elif provider == 'mistral':
        # Latestç³»ãŒæœ€å„ªå…ˆ
        if 'large-latest' in model_lower:
            score += 1000
        elif 'medium-latest' in model_lower:
            score += 900
        elif 'small-latest' in model_lower:
            score += 850
        elif 'latest' in model_lower:
            score += 800
            
    return score

def select_best_models(models: List[str], provider: str) -> tuple[Optional[str], Optional[str]]:
    """ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‹ã‚‰æœ€é©ãªé€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„ã«é¸æŠ"""
    if not models:
        return None, None
    
    # å„ãƒ¢ãƒ‡ãƒ«ã«ã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸
    scored_models = [(model, get_model_priority_score(model, provider)) for model in models]
    scored_models.sort(key=lambda x: x[1], reverse=True)
    
    # é€šå¸¸ãƒ¢ãƒ‡ãƒ«ï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ï¼‰
    normal_model = scored_models[0][0]
    
    # é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆç•°ãªã‚‹ç‰¹æ€§ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆï¼‰
    fast_model = None
    normal_lower = normal_model.lower()
    
    for model, score in scored_models:
        model_lower = model.lower()
        
        # é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ç•°ãªã‚‹ç‰¹æ€§ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚’é«˜é€Ÿç‰ˆã¨ã—ã¦é¸æŠ
        if provider == 'openai':
            # é€šå¸¸ãŒGPT-5åŸºæœ¬ç‰ˆãªã‚‰ã€chat-latestã¾ãŸã¯mini/nanoç‰ˆã‚’é¸æŠ
            if 'gpt-5' in normal_lower and 'gpt-5' in model_lower:
                if ('chat-latest' in model_lower and model_lower != normal_model) or \
                   ('mini' in model_lower and 'mini' not in normal_lower) or \
                   ('nano' in model_lower and 'nano' not in normal_lower):
                    fast_model = model
                    break
        elif provider == 'anthropic':
            # é€šå¸¸ãŒOpusãªã‚‰ã€Haikuã‚’é¸æŠ
            if 'haiku' in model_lower and 'haiku' not in normal_lower:
                fast_model = model
                break
        elif provider == 'grok':
            # é€šå¸¸ãŒGrok-4ãªã‚‰ã€mini-fastã‚’å„ªå…ˆé¸æŠ
            if ('fast' in model_lower or 'mini' in model_lower) and model_lower != normal_model:
                # grok-3-mini-fastã‚’æœ€å„ªå…ˆ
                if 'grok-3-mini-fast' in model_lower:
                    fast_model = model
                    break
                elif 'mini' in model_lower and 'fast' in model_lower:
                    fast_model = model
                    break
                elif 'fast' in model_lower:
                    if not fast_model:  # ã¾ã è¦‹ã¤ã‹ã£ã¦ã„ãªã„å ´åˆã®ã¿
                        fast_model = model
        elif provider == 'gemini':
            # é€šå¸¸ãŒProãªã‚‰ã€Flashã‚’é¸æŠ
            if ('flash' in model_lower and 'pro' in normal_lower) or \
               ('pro' in model_lower and 'flash' in normal_lower and score < scored_models[0][1]):
                fast_model = model
                break
        elif provider == 'groq':
            # é€šå¸¸ãŒ70bãªã‚‰ã€8b instantã‚’é¸æŠ
            if 'instant' in model_lower or ('8b' in model_lower and '70b' in normal_lower):
                fast_model = model
                break
        elif provider == 'mistral':
            # é€šå¸¸ãŒlargeãªã‚‰ã€smallã‚’é¸æŠ
            if 'small' in model_lower and 'large' in normal_lower:
                fast_model = model
                break
    
    # é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€2ç•ªç›®ã«é«˜ã„ã‚¹ã‚³ã‚¢ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    if not fast_model and len(scored_models) > 1:
        fast_model = scored_models[1][0]
    
    return normal_model, fast_model

def fetch_webpage_content(url, timeout=10):
    """ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ã™ã‚‹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ja;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"[ERROR] {url} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None
    """ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ã™ã‚‹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ja;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"[ERROR] {url} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def get_openai_models_from_web() -> Optional[tuple[str, str]]:
    """OpenAIã®å…¬é–‹æƒ…å ±ã‹ã‚‰æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å‹•çš„ã«å–å¾—"""
    try:
        print("      - Webã‚µã‚¤ãƒˆã‹ã‚‰è£œå®Œæƒ…å ±ã‚’å–å¾—ä¸­...")
        
        # è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
        sources = [
            ('https://platform.openai.com/docs/models', 5),
            ('https://openai.com/api/', 3),
            ('https://openai.com/pricing', 3),
        ]
        
        all_found_models = set()
        
        for url, timeout in sources:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Connection': 'keep-alive',
                }
                
                response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
                if response.status_code == 200:
                    content = response.text
                    
                    # ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                    patterns = [
                        r'gpt-[5-9][a-zA-Z0-9\-\.]*',  # GPT-5ä»¥ä¸Š
                        r'gpt-4[a-zA-Z0-9\-\.]*',      # GPT-4ç³»
                    ]
                    
                    for pattern in patterns:
                        models = re.findall(pattern, content, re.IGNORECASE)
                        for model in models:
                            if len(model) < 50:  # ç•°å¸¸ã«é•·ã„ãƒ¢ãƒ‡ãƒ«åã‚’é™¤å¤–
                                all_found_models.add(model.lower())
                
            except requests.RequestException:
                continue  # æ¬¡ã®ã‚½ãƒ¼ã‚¹ã‚’è©¦è¡Œ
        
        if all_found_models:
            model_list = list(all_found_models)
            print(f"      âœ“ Webæ¤œå‡ºãƒ¢ãƒ‡ãƒ«: {sorted(model_list)}")
            
            # å‹•çš„é¸æŠã‚’ä½¿ç”¨
            normal_model, fast_model = select_best_models(model_list, 'openai')
            
            if normal_model and fast_model:
                print(f"      âœ“ Webå‹•çš„é¸æŠ: {normal_model} / {fast_model}")
                return (normal_model, fast_model)
        
        # é«˜é€Ÿãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print("      - Webå–å¾—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€æ—¢çŸ¥ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
        return ('gpt-5', 'gpt-5-chat-latest')
        
    except Exception as e:
        print(f"      - Webå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return ('gpt-5', 'gpt-5-chat-latest')

def get_openai_models_via_api() -> tuple[Optional[str], Optional[str]]:
    """OpenAI APIçµŒç”±ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆé€šå¸¸ãƒ¢ãƒ‡ãƒ«ã€é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼‰"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            print("  - OpenAI API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None, None
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        # ã¾ãšAPIçµŒç”±ã§ç¢ºå®Ÿã«å–å¾—ã‚’è©¦è¡Œ
        print("    - OpenAI APIã‹ã‚‰ç›´æ¥ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ä¸­...")
        response = requests.get('https://api.openai.com/v1/models', headers=headers, timeout=5)
        if response.status_code == 200:
            models = response.json()
            gpt_models = [m['id'] for m in models['data'] if m['id'].startswith('gpt-')]
            
            print(f"    âœ“ APIçµŒç”±ã§ {len(gpt_models)} å€‹ã®GPTãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹")
            
            # è©³ç´°è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆç’°å¢ƒå¤‰æ•°VERBOSE_MODEãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
            verbose_mode = os.getenv('VERBOSE_MODE', '').lower() in ['true', '1', 'yes']
            
            # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤ºï¼ˆå¤šã™ãã‚‹å ´åˆã¯è¦ç´„ï¼‰
            if verbose_mode:
                print(f"    âœ“ å…¨GPTãƒ¢ãƒ‡ãƒ«ä¸€è¦§: {sorted(gpt_models)}")
            elif len(gpt_models) <= 10:
                print(f"    âœ“ ç™ºè¦‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {gpt_models}")
            else:
                # ä¸»è¦ãƒ¢ãƒ‡ãƒ«ã®ã¿è¡¨ç¤º
                important_models = [m for m in gpt_models if any(keyword in m for keyword in ['gpt-5', 'gpt-4o', 'gpt-4-turbo'])]
                other_count = len(gpt_models) - len(important_models)
                print(f"    âœ“ ä¸»è¦ãƒ¢ãƒ‡ãƒ«: {sorted(important_models)}")
                if other_count > 0:
                    print(f"    âœ“ ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«: {other_count}å€‹ (VERBOSE_MODE=true ã§å…¨è¡¨ç¤º)")
            
            # å‹•çš„ãªãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ä½¿ç”¨
            normal_model, fast_model = select_best_models(gpt_models, 'openai')
            
            if normal_model:
                print(f"    âœ“ {normal_model} ã‚’é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            if fast_model:
                print(f"    âœ“ {fast_model} ã‚’é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            
            return normal_model, fast_model
        
        else:
            print(f"    - API ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
            
        # APIå¤±æ•—æ™‚ã®ã¿Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦è¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ãï¼‰
        print("    - APIå¤±æ•—ã€Webã‚µã‚¤ãƒˆã‹ã‚‰æƒ…å ±å–å¾—ã‚’è©¦è¡Œ...")
        web_models = get_openai_models_from_web()
        if web_models:
            return web_models
        
    except Exception as e:
        print(f"  - OpenAI API ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None, None

def get_gemini_models_via_api() -> tuple[Optional[str], Optional[str]]:
    """Google APIçµŒç”±ã§åˆ©ç”¨å¯èƒ½ãªGeminiãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆé€šå¸¸ãƒ¢ãƒ‡ãƒ«ã€é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼‰"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key:
            print("  - Google API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None, None
        
        url = f'https://generativelanguage.googleapis.com/v1/models?key={api_key}'
        response = requests.get(url, timeout=5)
        
        if response.status_code == 200:
            models = response.json()
            gemini_models = [m['name'].split('/')[-1] for m in models.get('models', []) 
                           if 'gemini' in m['name'].lower()]
            
            print(f"    âœ“ {len(gemini_models)} å€‹ã®Geminiãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹: {gemini_models}")
            
            # å‹•çš„ãªãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ä½¿ç”¨
            normal_model, fast_model = select_best_models(gemini_models, 'gemini')
            
            if normal_model:
                print(f"    âœ“ {normal_model} ã‚’é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            if fast_model:
                print(f"    âœ“ {fast_model} ã‚’é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            
            return normal_model, fast_model
        
    except Exception as e:
        print(f"  - Google API ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None, None

def get_groq_models_via_api() -> tuple[Optional[str], Optional[str]]:
    """Groq APIçµŒç”±ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆé€šå¸¸ãƒ¢ãƒ‡ãƒ«ã€é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼‰"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        api_key = os.getenv('GROQ_API_KEY')
        if not api_key:
            print("  - Groq API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None, None
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        response = requests.get('https://api.groq.com/openai/v1/models', headers=headers, timeout=5)
        if response.status_code == 200:
            models = response.json()
            llama_models = [m['id'] for m in models['data'] if 'llama' in m['id'].lower()]
            
            print(f"    âœ“ {len(llama_models)} å€‹ã®Llamaãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹: {llama_models}")
            
            # å‹•çš„ãªãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ä½¿ç”¨
            normal_model, fast_model = select_best_models(llama_models, 'groq')
            
            if normal_model:
                print(f"    âœ“ {normal_model} ã‚’é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            if fast_model:
                print(f"    âœ“ {fast_model} ã‚’é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            
            return normal_model, fast_model
        
    except Exception as e:
        print(f"  - Groq API ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None, None

def get_anthropic_models_from_web() -> Optional[tuple[str, str]]:
    """Anthropicã®Webã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å‹•çš„ã«å–å¾—"""
    try:
        print("    - Anthropicã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ä¸­...")
        
        sources = [
            'https://docs.anthropic.com/claude/docs/models-overview',
            'https://docs.anthropic.com/claude/reference/getting-started-with-the-api',
            'https://www.anthropic.com/api',
            'https://www.anthropic.com/claude'
        ]
        
        all_found_models = set()
        
        for url in sources:
            try:
                content = fetch_webpage_content(url, timeout=5)
                if content:
                    # ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªClaudeãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
                    patterns = [
                        # Claude 4ç³»
                        r'claude-[4-9][^"\'>\s]*',
                        r'claude-opus-[4-9][^"\'>\s]*',
                        r'claude-sonnet-[4-9][^"\'>\s]*',
                        r'claude-haiku-[4-9][^"\'>\s]*',
                        # Claude 3ç³»
                        r'claude-3[^"\'>\s]*',
                        # æ—¥ä»˜ä»˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³
                        r'claude-[^"\'>\s]*-202[4-9][^"\'>\s]*',
                        # Latestç³»
                        r'claude-[^"\'>\s]*-latest[^"\'>\s]*'
                    ]
                    
                    for pattern in patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        for match in matches:
                            # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»ã—ã€å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                            clean_match = re.sub(r'[^\w\-\.]', '', match)
                            if (5 < len(clean_match) < 50 and 
                                'claude' in clean_match.lower() and
                                not any(invalid in clean_match.lower() for invalid in ['test', 'example', 'placeholder'])):
                                all_found_models.add(clean_match)
                                
            except Exception:
                continue  # æ¬¡ã®ã‚½ãƒ¼ã‚¹ã‚’è©¦è¡Œ
        
        if all_found_models:
            model_list = list(all_found_models)
            print(f"    âœ“ Anthropicæ¤œå‡ºãƒ¢ãƒ‡ãƒ«: {sorted(model_list)}")
            
            # å‹•çš„ãªãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ä½¿ç”¨
            normal_model, fast_model = select_best_models(model_list, 'anthropic')
            
            if normal_model:
                print(f"    âœ“ Anthropicå‹•çš„é¸æŠ: {normal_model} / {fast_model or 'ãªã—'}")
                return (normal_model, fast_model)
        
        print("    - ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰ã®è‡ªå‹•æ¤œå‡ºã¯å¤±æ•—")
        return None
        
    except Exception as e:
        print(f"    - ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_together_models_from_web() -> Optional[tuple[str, str]]:
    """Together AIã®Webã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å‹•çš„ã«å–å¾—"""
    try:
        print("    - Together AIã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ä¸­...")
        
        sources = [
            'https://docs.together.ai/docs/inference-models',
            'https://api.together.xyz/models',
            'https://together.ai/models'
        ]
        
        all_found_models = set()
        
        for url in sources:
            try:
                content = fetch_webpage_content(url, timeout=5)
                if content:
                    # ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªLlamaãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
                    patterns = [
                        r'meta-llama/Llama-[4-9][^"\'>\s]*',
                        r'meta-llama/Meta-Llama-[4-9][^"\'>\s]*',
                        r'meta-llama/llama-[4-9][^"\'>\s]*',
                        r'llamafactory/[^"\'>\s]*',
                        r'togethercomputer/[^"\'>\s]*'
                    ]
                    
                    for pattern in patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        for match in matches:
                            if len(match) < 100:  # ç•°å¸¸ã«é•·ã„åå‰ã‚’é™¤å¤–
                                all_found_models.add(match)
                                
            except Exception:
                continue  # æ¬¡ã®ã‚½ãƒ¼ã‚¹ã‚’è©¦è¡Œ
        
        if all_found_models:
            model_list = list(all_found_models)
            print(f"    âœ“ Together AIæ¤œå‡ºãƒ¢ãƒ‡ãƒ«: {sorted(model_list[:5])}...")  # æœ€åˆã®5ã¤ã‚’è¡¨ç¤º
            
            # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã§æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            scored_models = []
            for model in model_list:
                score = 0
                model_lower = model.lower()
                
                # Llama 4ç³»ã‚’æœ€å„ªå…ˆ
                if 'llama-4' in model_lower or 'llama/4' in model_lower:
                    score += 1000
                elif 'llama-3' in model_lower:
                    score += 800
                
                # æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å„ªå…ˆ
                if any(version in model_lower for version in ['instruct', 'chat', 'turbo']):
                    score += 100
                
                # FP8ãªã©ã®æœ€é©åŒ–ç‰ˆã‚’å„ªå…ˆ
                if any(opt in model_lower for opt in ['fp8', 'int8', 'awq']):
                    score += 50
                
                scored_models.append((model, score))
            
            scored_models.sort(key=lambda x: x[1], reverse=True)
            
            if scored_models:
                best_model = scored_models[0][0]
                print(f"    âœ“ Together AIå‹•çš„é¸æŠ: {best_model}")
                return (best_model, '')  # é«˜é€Ÿç‰ˆã¯é€šå¸¸è¨­å®šãªã—
        
        print("    - Together AIã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰ã®è‡ªå‹•æ¤œå‡ºã¯å¤±æ•—")
        return None
        
    except Exception as e:
        print(f"    - Together AI ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_grok_models_via_api() -> tuple[Optional[str], Optional[str]]:
    """xAI (Grok) APIçµŒç”±ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆé€šå¸¸ãƒ¢ãƒ‡ãƒ«ã€é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼‰"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        api_key = os.getenv('XAI_API_KEY')
        if not api_key:
            print("  - xAI API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None, None
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        # ã¾ãšè¨€èªãƒ¢ãƒ‡ãƒ«å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦è¡Œï¼ˆã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œã‚‹ï¼‰
        print("    - xAI APIã‹ã‚‰è¨€èªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ä¸­...")
        response = requests.get('https://api.x.ai/v1/language-models', headers=headers, timeout=5)
        
        if response.status_code == 200:
            models = response.json()
            print(f"    âœ“ /v1/language-models ã‹ã‚‰å–å¾—æˆåŠŸ")
            # language-modelsã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã¯modelsãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
            grok_models = [m['id'] for m in models.get('models', []) if 'grok' in m['id'].lower()]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ±ç”¨modelsã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦è¡Œ
            print("    - /v1/models ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦è¡Œ...")
            response = requests.get('https://api.x.ai/v1/models', headers=headers, timeout=5)
            if response.status_code == 200:
                models = response.json()
                print(f"    âœ“ /v1/models ã‹ã‚‰å–å¾—æˆåŠŸ")
                # modelsã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã¯dataãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
                grok_models = [m['id'] for m in models.get('data', []) if 'grok' in m['id'].lower()]
            else:
                print(f"    - API ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
                return None, None
        
        if grok_models:
            print(f"    âœ“ {len(grok_models)} å€‹ã®Grokãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹: {grok_models}")
            
            # å‹•çš„ãªãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ä½¿ç”¨
            normal_model, fast_model = select_best_models(grok_models, 'grok')
            
            if normal_model:
                print(f"    âœ“ {normal_model} ã‚’é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            if fast_model:
                print(f"    âœ“ {fast_model} ã‚’é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            
            return normal_model, fast_model
        else:
            print("    - Grokãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None, None
        
    except Exception as e:
        print(f"  - xAI API ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None, None

def test_anthropic_model(api_key: str, model: str) -> bool:
    """Anthropicãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒ†ã‚¹ãƒˆ"""
    try:
        headers = {
            'x-api-key': api_key,
            'Content-Type': 'application/json',
            'anthropic-version': '2023-06-01'
        }
        
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ç¢ºèª
        data = {
            'model': model,
            'max_tokens': 1,
            'messages': [{'role': 'user', 'content': 'Hi'}]
        }
        
        response = requests.post(
            'https://api.anthropic.com/v1/messages',
            headers=headers,
            json=data,
            timeout=5
        )
        
        # 200ç³»ã¾ãŸã¯é©åˆ‡ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ãªã‚‰åˆ©ç”¨å¯èƒ½
        return response.status_code in [200, 400]  # 400ã¯max_tokensãŒå°‘ãªã™ãã‚‹å ´åˆã§ã‚‚ãƒ¢ãƒ‡ãƒ«ã¯æœ‰åŠ¹
        
    except Exception:
        return False

def get_anthropic_models_via_api() -> tuple[Optional[str], Optional[str]]:
    """Anthropic APIçµŒç”±ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆé€šå¸¸ãƒ¢ãƒ‡ãƒ«ã€é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼‰"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            print("  - Anthropic API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None, None
        
        headers = {
            'x-api-key': api_key,
            'Content-Type': 'application/json',
            'anthropic-version': '2023-06-01'
        }
        
        # ã¾ãšWebã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ã‚’è©¦è¡Œ
        web_models = get_anthropic_models_from_web()
        if web_models:
            return web_models
        
        # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå¤±æ•—ã—ãŸå ´åˆã€APIãƒ†ã‚¹ãƒˆå‘¼ã³å‡ºã—ã§åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
        print("    - APIãƒ†ã‚¹ãƒˆå‘¼ã³å‡ºã—ã§åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­...")
        
        # å‹•çš„ã«ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        potential_models = []
        
        # Claude 4ç³»ã®å¯èƒ½æ€§ã‚’æ¢ã‚‹
        for version in ['4', '4-opus', 'opus-4']:
            for date in ['20250514', '20250301', '20241201', '20241001']:
                potential_models.append(f'claude-{version}-{date}')
                potential_models.append(f'claude-{version.replace("-", "-")}-{date}')
        
        # Claude 3.7ç³»
        for date in ['20250219', '20250101', '20241201']:
            potential_models.append(f'claude-3-7-sonnet-{date}')
        
        # Claude 3.5ç³»
        for variant in ['sonnet', 'haiku']:
            for date in ['latest', '20241022', '20241001', '20240620']:
                potential_models.append(f'claude-3-5-{variant}-{date}')
        
        # æ—¢çŸ¥ã®å®‰å®šç‰ˆã‚‚è¿½åŠ 
        potential_models.extend([
            'claude-opus-4-20250514', 'claude-sonnet-4-20250514', 
            'claude-3-7-sonnet-20250219', 'claude-3-5-sonnet-20241022',
            'claude-3-5-haiku-latest', 'claude-3-5-haiku-20241022'
        ])
        
        # é‡è¤‡ã‚’å‰Šé™¤
        test_models = list(set(potential_models))
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒ†ã‚¹ãƒˆ
        available_models = []
        for model in test_models:
            if test_anthropic_model(api_key, model):
                available_models.append(model)
                print(f"    âœ“ {model} - åˆ©ç”¨å¯èƒ½")
            else:
                print(f"    âœ— {model} - åˆ©ç”¨ä¸å¯")
        
        if available_models:
            # å‹•çš„ãªãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ä½¿ç”¨
            normal_model, fast_model = select_best_models(available_models, 'anthropic')
            
            if normal_model:
                print(f"    âœ“ {normal_model} ã‚’é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            if fast_model:
                print(f"    âœ“ {fast_model} ã‚’é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            
            return normal_model, fast_model
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æ—¢çŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™
        return 'claude-opus-4-20250514', 'claude-3-5-haiku-latest'
        
    except Exception as e:
        print(f"  - Anthropic API ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None, None

def get_mistral_models_via_api() -> tuple[Optional[str], Optional[str]]:
    """Mistral APIçµŒç”±ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆé€šå¸¸ãƒ¢ãƒ‡ãƒ«ã€é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼‰"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        api_key = os.getenv('MISTRAL_API_KEY')
        if not api_key:
            print("  - Mistral API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None, None
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        response = requests.get('https://api.mistral.ai/v1/models', headers=headers, timeout=5)
        if response.status_code == 200:
            models = response.json()
            mistral_models = [m['id'] for m in models['data']]
            
            print(f"    âœ“ {len(mistral_models)} å€‹ã®Mistralãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹: {mistral_models}")
            
            # å‹•çš„ãªãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ä½¿ç”¨
            normal_model, fast_model = select_best_models(mistral_models, 'mistral')
            
            if normal_model:
                print(f"    âœ“ {normal_model} ã‚’é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            if fast_model:
                print(f"    âœ“ {fast_model} ã‚’é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸æŠï¼ˆå‹•çš„é¸æŠï¼‰")
            
            return normal_model, fast_model
        
    except Exception as e:
        print(f"  - Mistral API ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None, None

def get_latest_models():
    """å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
    print("[INFO] æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ä¸­...")
    
    latest_models = {}
    
    # OpenAI
    print("OpenAIã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­...")
    openai_normal, openai_fast = get_openai_models_via_api()
    if openai_normal:
        latest_models['ChatGPT'] = {'model': openai_normal, 'fast_model': openai_fast}
        print(f"  - OpenAIæœ€æ–°ãƒ¢ãƒ‡ãƒ«: {openai_normal} / {openai_fast}")
    else:
        print("  - OpenAIæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—ã€å‰å›è¨­å®šå€¤ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨")
        if 'ChatGPT' in FALLBACK_MODELS:
            latest_models['ChatGPT'] = {
                'model': FALLBACK_MODELS['ChatGPT']['model'], 
                'fast_model': FALLBACK_MODELS['ChatGPT']['fast_model']
            }
            print(f"  - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤: {FALLBACK_MODELS['ChatGPT']['model']} / {FALLBACK_MODELS['ChatGPT']['fast_model']}")
    
    # Gemini
    print("Geminiã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­...")
    gemini_normal, gemini_fast = get_gemini_models_via_api()
    if gemini_normal:
        latest_models['Gemini'] = {'model': gemini_normal, 'fast_model': gemini_fast}
        print(f"  - Geminiæœ€æ–°ãƒ¢ãƒ‡ãƒ«: {gemini_normal} / {gemini_fast}")
    else:
        print("  - Geminiæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—ã€å‰å›è¨­å®šå€¤ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨")
        if 'Gemini' in FALLBACK_MODELS:
            latest_models['Gemini'] = {
                'model': FALLBACK_MODELS['Gemini']['model'], 
                'fast_model': FALLBACK_MODELS['Gemini']['fast_model']
            }
            print(f"  - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤: {FALLBACK_MODELS['Gemini']['model']} / {FALLBACK_MODELS['Gemini']['fast_model']}")
    
    # Groq
    print("Groqã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­...")
    groq_normal, groq_fast = get_groq_models_via_api()
    if groq_normal:
        latest_models['Groq'] = {'model': groq_normal, 'fast_model': groq_fast}
        print(f"  - Groqæœ€æ–°ãƒ¢ãƒ‡ãƒ«: {groq_normal} / {groq_fast}")
    else:
        print("  - Groqæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—ã€å‰å›è¨­å®šå€¤ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨")
        if 'Groq' in FALLBACK_MODELS:
            latest_models['Groq'] = {
                'model': FALLBACK_MODELS['Groq']['model'], 
                'fast_model': FALLBACK_MODELS['Groq']['fast_model']
            }
            print(f"  - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤: {FALLBACK_MODELS['Groq']['model']} / {FALLBACK_MODELS['Groq']['fast_model']}")
    
    # Mistral
    print("Mistralã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­...")
    mistral_normal, mistral_fast = get_mistral_models_via_api()
    if mistral_normal:
        latest_models['Mistral'] = {'model': mistral_normal, 'fast_model': mistral_fast}
        print(f"  - Mistralæœ€æ–°ãƒ¢ãƒ‡ãƒ«: {mistral_normal} / {mistral_fast}")
    else:
        print("  - Mistralæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—ã€å‰å›è¨­å®šå€¤ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨")
        if 'Mistral' in FALLBACK_MODELS:
            latest_models['Mistral'] = {
                'model': FALLBACK_MODELS['Mistral']['model'], 
                'fast_model': FALLBACK_MODELS['Mistral']['fast_model']
            }
            print(f"  - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤: {FALLBACK_MODELS['Mistral']['model']} / {FALLBACK_MODELS['Mistral']['fast_model']}")
    
    # Anthropic
    print("Anthropicã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­...")
    anthropic_normal, anthropic_fast = get_anthropic_models_via_api()
    if anthropic_normal:
        latest_models['Anthropic'] = {'model': anthropic_normal, 'fast_model': anthropic_fast}
        print(f"  - Anthropicæœ€æ–°ãƒ¢ãƒ‡ãƒ«: {anthropic_normal} / {anthropic_fast}")
    else:
        print("  - Anthropicæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—ã€å‰å›è¨­å®šå€¤ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨")
        if 'Anthropic' in FALLBACK_MODELS:
            latest_models['Anthropic'] = {
                'model': FALLBACK_MODELS['Anthropic']['model'], 
                'fast_model': FALLBACK_MODELS['Anthropic']['fast_model']
            }
            print(f"  - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤: {FALLBACK_MODELS['Anthropic']['model']} / {FALLBACK_MODELS['Anthropic']['fast_model']}")
    
    # Together AIï¼ˆã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦è¡Œï¼‰
    print("Together AIã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­...")
    together_models = get_together_models_from_web()
    if together_models:
        latest_models['Together'] = {'model': together_models[0], 'fast_model': together_models[1]}
        print(f"  - Together AIæœ€æ–°ãƒ¢ãƒ‡ãƒ«: {together_models[0]} / {together_models[1]}")
    else:
        print("  - Together AIæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—ã€å‰å›è¨­å®šå€¤ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨")
        if 'Together' in FALLBACK_MODELS:
            latest_models['Together'] = {
                'model': FALLBACK_MODELS['Together']['model'], 
                'fast_model': FALLBACK_MODELS['Together']['fast_model']
            }
            print(f"  - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤: {FALLBACK_MODELS['Together']['model']} / {FALLBACK_MODELS['Together']['fast_model']}")
    
    # Grokï¼ˆxAIï¼‰ï¼ˆAPIã‚’è©¦è¡Œï¼‰
    print("Grokã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­...")
    grok_normal, grok_fast = get_grok_models_via_api()
    if grok_normal:
        latest_models['Grok'] = {'model': grok_normal, 'fast_model': grok_fast}
        print(f"  - Grokæœ€æ–°ãƒ¢ãƒ‡ãƒ«: {grok_normal} / {grok_fast}")
    else:
        print("  - Grokæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã«å¤±æ•—ã€å‰å›è¨­å®šå€¤ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨")
        if 'Grok' in FALLBACK_MODELS:
            latest_models['Grok'] = {
                'model': FALLBACK_MODELS['Grok']['model'], 
                'fast_model': FALLBACK_MODELS['Grok']['fast_model']
            }
            print(f"  - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤: {FALLBACK_MODELS['Grok']['model']} / {FALLBACK_MODELS['Grok']['fast_model']}")
    
    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ã®é©ç”¨ï¼ˆæ–°ã—ã„å€¤ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆï¼‰
    for provider in ['OpenAI', 'Mistral AI', 'Google', 'Anthropic', 'Together', 'Grok']:
        if provider not in latest_models:
            if provider in FALLBACK_MODELS:
                latest_models[provider] = {
                    'model': FALLBACK_MODELS[provider]['model'], 
                    'fast_model': FALLBACK_MODELS[provider]['fast_model']
                }
                print(f"  - {provider}: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ã‚’ä½¿ç”¨ - {FALLBACK_MODELS[provider]['model']} / {FALLBACK_MODELS[provider]['fast_model']}")
            else:
                # æœ€çµ‚çš„ãªãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                hardcoded_defaults = get_hardcoded_fallback_models()
                if provider in hardcoded_defaults:
                    latest_models[provider] = {
                        'model': hardcoded_defaults[provider]['model'], 
                        'fast_model': hardcoded_defaults[provider]['fast_model']
                    }
                    print(f"  - {provider}: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ - {hardcoded_defaults[provider]['model']} / {hardcoded_defaults[provider]['fast_model']}")
    
    return latest_models

def update_config_file(latest_models):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    print("[INFO] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ä¸­...")
    
    # ç¾åœ¨ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿
    current_config = {}
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    current_config[row['assistant_name']] = {
                        'module': row['module'],
                        'class': row['class'],
                        'model': row['model'],
                        'fast_model': row.get('fast_model', '')  # fast_modelãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºæ–‡å­—
                    }
        except Exception as e:
            print(f"[ERROR] ç¾åœ¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return False
    
    # æ–°ã—ã„è¨­å®šã‚’ä½œæˆ
    new_config = {}
    updated = False
    
    for assistant_name, fallback_info in FALLBACK_MODELS.items():
        if assistant_name in current_config:
            # ç¾åœ¨ã®è¨­å®šã‚’ãƒ™ãƒ¼ã‚¹ã«æ›´æ–°
            new_config[assistant_name] = current_config[assistant_name].copy()
        else:
            # æ–°è¦è¿½åŠ 
            new_config[assistant_name] = {
                'module': fallback_info['module'],
                'class': fallback_info['class'],
                'model': fallback_info['model'],
                'fast_model': fallback_info['fast_model']
            }
        
        # æœ€æ–°ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°æ›´æ–°
        if assistant_name in latest_models:
            latest_info = latest_models[assistant_name]
            
            # é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
            old_model = new_config[assistant_name]['model']
            new_model = latest_info['model']
            if old_model != new_model:
                new_config[assistant_name]['model'] = new_model
                print(f"  - {assistant_name} (model): {old_model} â†’ {new_model}")
                updated = True
            
            # é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
            old_fast_model = new_config[assistant_name].get('fast_model', '')
            new_fast_model = latest_info.get('fast_model', '')
            if old_fast_model != new_fast_model and new_fast_model is not None:
                new_config[assistant_name]['fast_model'] = new_fast_model
                print(f"  - {assistant_name} (fast_model): {old_fast_model} â†’ {new_fast_model}")
                updated = True
            
            if old_model == new_model and old_fast_model == new_fast_model:
                print(f"  - {assistant_name}: {old_model} / {old_fast_model} (å¤‰æ›´ãªã—)")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            fallback_model = fallback_info['model']
            fallback_fast_model = fallback_info['fast_model']
            
            if new_config[assistant_name]['model'] != fallback_model:
                new_config[assistant_name]['model'] = fallback_model
                print(f"  - {assistant_name} (model): ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ« {fallback_model} ã‚’ä½¿ç”¨")
                updated = True
            
            if new_config[assistant_name].get('fast_model', '') != fallback_fast_model:
                new_config[assistant_name]['fast_model'] = fallback_fast_model
                print(f"  - {assistant_name} (fast_model): ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ {fallback_fast_model} ã‚’ä½¿ç”¨")
                updated = True
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã¿
    if updated or not os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['assistant_name', 'module', 'class', 'model', 'fast_model'])
                for assistant_name, config in new_config.items():
                    writer.writerow([
                        assistant_name, 
                        config['module'], 
                        config['class'], 
                        config['model'],
                        config.get('fast_model', '')
                    ])
            print(f"[INFO] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {CONFIG_FILE} ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
            return True
        except Exception as e:
            print(f"[ERROR] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—: {e}")
            return False
    else:
        print("[INFO] æ›´æ–°ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“")
        return True

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    global FALLBACK_MODELS
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®å‡¦ç†
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        # ã‚³ã‚¹ãƒˆç®¡ç†ã‚³ãƒãƒ³ãƒ‰
        if command == "costs":
            if len(sys.argv) < 3:
                print("ä½¿ç”¨æ³•: python update_ai_config.py costs [list|verify|add|update|crawl]")
                return 1
                
            subcommand = sys.argv[2]
            
            if subcommand == "list":
                list_costs()
            elif subcommand == "verify":
                verify_provider_mapping()
            elif subcommand == "add":
                if len(sys.argv) < 7:
                    print("ä½¿ç”¨æ³•: python update_ai_config.py costs add <model> <provider> <input_cost> <output_cost> [notes]")
                    return 1
                model = sys.argv[3]
                provider = sys.argv[4]
                try:
                    input_cost = float(sys.argv[5])
                    output_cost = float(sys.argv[6])
                except ValueError:
                    print("âŒ ã‚³ã‚¹ãƒˆã¯æ•°å€¤ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    return 1
                notes = " ".join(sys.argv[7:]) if len(sys.argv) > 7 else ""
                add_model_cost(model, provider, input_cost, output_cost, notes)
            elif subcommand == "update":
                if len(sys.argv) < 6:
                    print("ä½¿ç”¨æ³•: python update_ai_config.py costs update <model> <input_cost> <output_cost>")
                    return 1
                model = sys.argv[3]
                try:
                    input_cost = float(sys.argv[4])
                    output_cost = float(sys.argv[5])
                except ValueError:
                    print("âŒ ã‚³ã‚¹ãƒˆã¯æ•°å€¤ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    return 1
                update_model_cost(model, input_cost, output_cost)
            elif subcommand == "crawl":
                if len(sys.argv) < 4:
                    print("ä½¿ç”¨æ³•: python update_ai_config.py costs crawl <source>")
                    print("åˆ©ç”¨å¯èƒ½ãªã‚½ãƒ¼ã‚¹: openai, anthropic, google, groq, all")
                    return 1
                source = sys.argv[3]
                crawl_costs(source)
            else:
                print("âŒ ä¸æ˜ãªã‚³ã‚¹ãƒˆã‚³ãƒãƒ³ãƒ‰ã§ã™ã€‚")
                print("åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰: list, verify, add, update, crawl")
                return 1
            return 0
        elif command == "update":
            # AIè¨­å®šæ›´æ–°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œã¨åŒã˜ï¼‰
            pass
        else:
            print("âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰ã§ã™ã€‚")
            print("åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰: update (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ), costs")
            return 1
    
    # AIè¨­å®šæ›´æ–°å‡¦ç†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œï¼‰
    print("=== AI Assistants Configuration Updater ===")
    print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
    print("[INFO] API ã‚­ãƒ¼ã®ç¢ºèª...")
    api_keys = {
        'OpenAI': os.getenv('OPENAI_API_KEY'),
        'Google': os.getenv('GOOGLE_API_KEY'),
        'Groq': os.getenv('GROQ_API_KEY'),
        'Mistral': os.getenv('MISTRAL_API_KEY'),
        'Together': os.getenv('TOGETHER_API_KEY'),
        'Anthropic': os.getenv('ANTHROPIC_API_KEY'),
        'xAI': os.getenv('XAI_API_KEY')
    }
    
    for provider, key in api_keys.items():
        status = "âœ“" if key else "âœ—"
        print(f"  {status} {provider}_API_KEY: {'è¨­å®šæ¸ˆã¿' if key else 'æœªè¨­å®š'}")
    print()
    
    try:
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆã“ã®æ™‚ç‚¹ã§FALLBACK_MODELSãŒè¨­å®šã•ã‚Œã‚‹ï¼‰
        if not backup_config_file():
            response = input("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
            if response.lower() != 'y':
                print("å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ")
                return
        
        # æœ€æ–°ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—
        latest_models = get_latest_models()
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
        if update_config_file(latest_models):
            print()
            print("[SUCCESS] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«: {BACKUP_FILE}")
            
            # æ›´æ–°å¾Œã®è¨­å®šã‚’è¡¨ç¤º
            print("\n[INFO] æ›´æ–°å¾Œã®è¨­å®š:")
            if os.path.exists(CONFIG_FILE):
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        model = row['model']
                        fast_model = row.get('fast_model', '')
                        if fast_model:
                            print(f"  - {row['assistant_name']}: {model} / {fast_model}")
                        else:
                            print(f"  - {row['assistant_name']}: {model}")
        else:
            print()
            print("[ERROR] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return 1
            
    except KeyboardInterrupt:
        print("\n[INFO] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 1
    except Exception as e:
        print(f"[ERROR] äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
